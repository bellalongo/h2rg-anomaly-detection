{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c096552d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'jpl_env (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/ilongo/h2rg-anomaly-detection/jpl_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5392a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR = '/proj/case/2025-06-05'\n",
    "ALL_DIRS = os.listdir(DATA_ROOT_DIR)\n",
    "EUCLID_DIRS = []\n",
    "CASE_DIRS = []\n",
    "\n",
    "TOTAL_FRAMES = 450"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399dcba",
   "metadata": {},
   "source": [
    "#### Grab all directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b662a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'$RECYCLE.BIN'\t    18274_Euclid_SCA   FPM_102_101   FPM_EM3_EM6_post_vibe\n",
      " 18220_Euclid_SCA   18275_Euclid_SCA   FPM_102_105   FPM_EM3_EM6_pre_vibe\n",
      " 18248_Euclid_SCA   18282_Euclid_SCA   FPM_104_103  'System Volume Information'\n",
      " 18266_Euclid_SCA   18283_Euclid_SCA   FPM_EM1_EM2\n"
     ]
    }
   ],
   "source": [
    "!ls '/proj/case/2025-06-05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3543994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclid test data directories: ['18220_Euclid_SCA', '18248_Euclid_SCA', '18266_Euclid_SCA', '18274_Euclid_SCA', '18275_Euclid_SCA', '18282_Euclid_SCA', '18283_Euclid_SCA']\n",
      "\n",
      "Case test data directories: ['FPM_102_101', 'FPM_102_105', 'FPM_104_103', 'FPM_EM1_EM2', 'FPM_EM3_EM6_post_vibe', 'FPM_EM3_EM6_pre_vibe']\n"
     ]
    }
   ],
   "source": [
    "for dir in ALL_DIRS:\n",
    "    if 'Euclid' in dir:\n",
    "        EUCLID_DIRS.append(dir)\n",
    "    elif 'FPM' in dir:\n",
    "        CASE_DIRS.append(dir)\n",
    "\n",
    "print(f'Euclid test data directories: {EUCLID_DIRS}\\n')\n",
    "print(f'Case test data directories: {CASE_DIRS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32573dd4",
   "metadata": {},
   "source": [
    "#### Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingData:\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__():\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def create_directory():\n",
    "        \"\"\"\n",
    "            * basically create training set dir for preprocess to be stored as h5py\n",
    "        \"\"\"\n",
    "        # Make the training set directory\n",
    "        root = 'training_set'\n",
    "        curr_dir = Path(root).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Make difference array directory\n",
    "        Path(f'{root}/raw_differences').mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Make patches directory\n",
    "        Path(f'{root}/patches').mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        # Make temporal analysis directory\n",
    "        Path(f'{root}/temporal_analysis').mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "\n",
    "    def _preprocess(dataset_name):\n",
    "        \"\"\"\n",
    "            * basically do the first frame subtraction\n",
    "            * dataset_name = 'euclid' or 'case'\n",
    "        \"\"\"\n",
    "        if dataset_name.upper() == 'EUCLID':\n",
    "            print('ay')\n",
    "\n",
    "        elif dataset_name.upper() == 'CASE':\n",
    "            print('ay')\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f'Invalid dataset name of {dataset_name.upper()}, must be EUCLUD or CASE')\n",
    "        \n",
    "    \n",
    "    def _grab_filenames(path):\n",
    "        \"\"\"\n",
    "            * basically grab all filenames for the current path\n",
    "        \"\"\"\n",
    "        try:\n",
    "            entries = os.listdir(path)\n",
    "            filenames = [entry for entry in entries if os.path.isfile(os.path.join(path, entry))]\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            print('Folder not found at {path}')\n",
    "            return None\n",
    "        \n",
    "        return filenames\n",
    "    \n",
    "\n",
    "    def _subtract_ref_pixels(frame):\n",
    "        \"\"\"\n",
    "            * perform ref pixel subtraction as documented in paper\n",
    "                Reference Pixel Subtraction\n",
    "                Method for EUCLID SCS Noise\n",
    "                Specification\n",
    "                Bogna Kubik\n",
    "        \"\"\"\n",
    "        # Make sure shape is 2048 x 2048\n",
    "        if frame.shape != (2048, 2048):\n",
    "            raise ValueError(f'Invalid frame shape of {frame.shape}, must be (2048, 2048)')\n",
    "        \n",
    "        corrected_frame = frame.copy()\n",
    "\n",
    "        # Optimal window side via paper\n",
    "        x_opt = 64\n",
    "        y_opt = 4\n",
    "\n",
    "        # Reference pixel regions\n",
    "        up_ref = frame[0:4, :] # Top 4 rows\n",
    "        down_ref = frame[2044:2048, :] # Bottom 4 rows\n",
    "        left_ref = frame[:, 0:4] # Left 4 cols\n",
    "        right_ref = frame[:, 2044:2048]\n",
    "\n",
    "        # Process each of the 32 channels\n",
    "        for ch in range(32):\n",
    "            # Skip left ref pixels\n",
    "            if ch == 0:\n",
    "                col_start, col_end = 4, 64\n",
    "            \n",
    "            # Skip right ref pixels\n",
    "            elif ch == 31:\n",
    "                col_start, col_end = ch * 64, 2044  # Fixed: clearer intent\n",
    "\n",
    "            # Inner channels\n",
    "            else:\n",
    "                col_start, col_end = ch * 64, (ch + 1) * 64\n",
    "\n",
    "            # Up/down correction w/ extract channel specific ref pixels \n",
    "            for col in range(col_end - col_start):\n",
    "                global_col = col_start + col\n",
    "\n",
    "                # Sliding window for up/down\n",
    "                window_start = max(0, col_start)\n",
    "                window_end = min(2048, col_end)\n",
    "\n",
    "                # Average the up and down ref pixels\n",
    "                up_avg = np.mean(up_ref[:, window_start:window_end])\n",
    "                down_avg = np.mean(down_ref[:, window_start:window_end])\n",
    "\n",
    "                # Interpolate correction\n",
    "                slope = (up_avg - down_avg) / 2044\n",
    "\n",
    "                # Apply correction to each row in the column, skipping refs\n",
    "                for row in range(4, 2044):\n",
    "                    ref_correction = down_avg + (row - 1.5) * slope\n",
    "                    corrected_frame[row, global_col] -= ref_correction\n",
    "\n",
    "        # Correct left ref pixel\n",
    "        left_ref_corrected = left_ref.copy()\n",
    "        right_ref_corrected = right_ref.copy()\n",
    "\n",
    "        # Subtract the up/down correction from the left/right pixels\n",
    "        up_avg_full = np.mean(up_ref)\n",
    "        down_avg_full = np.mean(down_ref)\n",
    "        slope_full = (up_avg_full - down_avg_full) / 2044\n",
    "\n",
    "        # Fixed: Correct the left/right ref pixels with proper loop\n",
    "        for row in range(4, 2044):\n",
    "            ref_correction = down_avg_full + (row - 1.5) * slope_full\n",
    "            left_ref_corrected[row, :] -= ref_correction\n",
    "            right_ref_corrected[row, :] -= ref_correction\n",
    "\n",
    "        # Apply the correction using sliding window\n",
    "        for row in range(4, 2044):\n",
    "            # Sliding window for left/right\n",
    "            window_start = max(4, row - y_opt)\n",
    "            window_end = min(2044, row + y_opt + 1)\n",
    "\n",
    "            # Fixed: Average corrected left/right pixels (syntax error)\n",
    "            left_avg = np.mean(left_ref_corrected[window_start:window_end, :])\n",
    "            right_avg = np.mean(right_ref_corrected[window_start:window_end, :])  # Fixed the dot\n",
    "            lr_correction = (left_avg + right_avg) / 2\n",
    "\n",
    "            corrected_frame[row, 4:2044] -= lr_correction\n",
    "\n",
    "        return corrected_frame\n",
    "    \n",
    "    \n",
    "    def _compute_difference_fits(file_path):\n",
    "        \"\"\"\n",
    "            * subtract frame 0 from the current fits file\n",
    "        \"\"\"\n",
    "        frame_difs = []\n",
    "\n",
    "        # Open the current fits file\n",
    "        with fits.open(file_path) as hdul:\n",
    "            # Grab frame 0\n",
    "            frame_0 = hdul[1].data.astype(np.int32)\n",
    "\n",
    "            # Make \n",
    "\n",
    "            # Iterate through all frames, skipping the first\n",
    "            for i in range(2, TOTAL_FRAMES):\n",
    "                # Grab the current frame\n",
    "                curr_frame = hdul[i].data.astype(np.int32)\n",
    "\n",
    "                # Subtract from 0\n",
    "                dif = np.abs(curr_frame - frame_0)\n",
    "                frame_difs.append(dif)\n",
    "      \n",
    "    def _euclid_preprocess(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" \n",
    "        # Grab all euclid filenames\n",
    "        for dir in EUCLID_DIRS:\n",
    "            path = f'{DATA_ROOT_DIR}/{dir}'\n",
    "            filenames = self._grab_filenames(path)\n",
    "\n",
    "            # Iterate through all filenames\n",
    "\n",
    "\n",
    "    def load():\n",
    "        print('ay')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f1cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = TrainingData\n",
    "training.create_directory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
